---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## PROJECT RULES & CODING STANDARDS

Comprehensive validation must be added across all text input fields in the project wherever required. When using `useCallback` in TypeScript, ensure the correct syntax for return type annotations is used. The correct syntax is `useCallback((): boolean => { ... })` and not `useCallback(): boolean => { ... }`. Ensure all JSX components have corresponding closing tags and proper nesting. When authenticating users against a database without Supabase Auth accounts, the login function should query the users table directly by email. Password validation must be implemented with proper hashing and validation, and should not be disabled in production environments. User roles from the database should be properly mapped to application roles. When authenticating users against a database without Supabase Auth accounts, the login function must query the users table directly by email, skipping Supabase Auth. Password validation must be implemented (but can be temporarily disabled for local testing). Role mapping should handle roles such as admin, manager, salesperson, credit-ops, and compliance. The application must support both Supabase Auth users (new signups) and database-only users (existing users). Registration must create Supabase Auth users and sync to the database, assigning roles via the `user_roles` table. The registration form should offer roles that match the database: `salesperson`, `manager`, `credit-ops`, `admin`, and `auditor`, and `compliance`. The application must support creating a `super_admin` role during initial setup.

The `users` table includes the following fields: `id`, `full_name`, `email`, `mobile`, `email_verified_at`, `remember_token`, `department_id`, `employment_type_id`, `organization_id`, `status`, `metadata`, `created_at`, `updated_at`, `deleted_at`, `auth_id`, `role`. Passwords are handled by Supabase Auth, and the `auth_id` field links to Supabase Auth users. The `full_name` field should be used instead of separate first/last names. Use `mobile` instead of `phone`. The `role` field is stored directly in the `users` table.

The application uses a dual authentication system:
1.  Supabase Auth users (new signups with proper authentication)
2.  Database-only users (existing users without Supabase Auth accounts)

The `src/contexts/AuthContextFixed.tsx` file is the main authentication context for the Veriphy Bank frontend application. When importing `AuthContext`, ensure to import from `AuthContextFixed` due to the actual file name. The AuthContext supports registration functionality, including the creation of a `super_admin` user. The super admin is responsible for the initial organization setup using Supabase CRUD APIs. The admin can then configure their organization. The system should support adding one row to every table using CRUD operations in the super admin dashboard to populate the database initially. The `system_settings` table must exist in the Supabase database. The application must not use fallbacks if the `system_settings` table is missing; instead, the table must be created using the SQL script below.

To create the `system_settings` table in your Supabase database:

1.  Go to your Supabase Dashboard: <https://supabase.com/dashboard>
2.  Navigate to your project
3.  Go to **SQL Editor**
4.  Run the following SQL script to create the table:

```sql
-- Create system_settings table
CREATE TABLE IF NOT EXISTS system_settings (
    id BIGSERIAL PRIMARY KEY,
    key VARCHAR(100) NOT NULL UNIQUE,
    value TEXT,
    description TEXT,
    category VARCHAR(50) DEFAULT 'general',
    is_encrypted BOOLEAN DEFAULT false,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT true
);

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS idx_system_settings_key ON system_settings(key);
CREATE INDEX IF NOT EXISTS idx_system_settings_category ON system_settings(category);
CREATE INDEX IF NOT EXISTS idx_system_settings_active ON system_settings(is_active);
```

After creating the table, run this SQL to insert the default settings:

```sql
-- Insert default system settings
INSERT INTO system_settings (key, value, description, category) VALUES
('app_name', 'Veriphy Bank', 'Application name', 'general'),
('app_version', '1.0.0', 'Application version', 'general'),
('max_login_attempts', '5', 'Maximum login attempts before account lockout', 'security'),
('session_timeout_minutes', '30', 'Session timeout in minutes', 'security'),
('password_min_length', '8', 'Minimum password length', 'security'),
('password_require_special', 'true', 'Require special characters in password', 'security'),
('kyc_verification_required', 'true', 'KYC verification required for new accounts', 'security'),
('transaction_limit_daily', '100000', 'Daily transaction limit in INR', 'business'),
('transaction_limit_monthly', '1000000', 'Monthly transaction limit in INR', 'business'),
('whatsapp_enabled', 'true', 'WhatsApp integration enabled', 'integration'),
('audit_log_retention_days', '2555', 'Audit log retention period in days', 'system'),
('document_encryption_enabled', 'true', 'Document encryption enabled', 'security'),
('two_factor_auth_enabled', 'true', 'Two-factor authentication enabled', 'security'),
('maintenance_mode', 'false', 'System maintenance mode', 'system'),
('email_notifications_enabled', 'true', 'Email notifications enabled', 'notification'),
('sms_notifications_enabled', 'true', 'SMS notifications_enabled', 'notification'),
('push_notifications_enabled', 'true', 'Push notifications enabled', 'notification'),
('auto_logout_minutes', '60', 'Auto logout timeout in minutes', 'security'),
('password_reset_expiry_hours', '24', 'Password reset link expiry in hours', 'security'),
('file_upload_max_size_mb', '10', 'Maximum file upload size in MB', 'system'),
('supported_file_types', 'pdf,jpg,jpeg,png,doc,docx', 'Supported file types for uploads', 'system')
ON CONFLICT (key) DO NOTHING;
```

To verify the table was created and data was inserted, run:

```sql
-- Verify the table was created and data was inserted
SELECT 'system_settings table created successfully' as status;
SELECT COUNT(*) as total_settings FROM system_settings;
SELECT key, value, category FROM system_settings ORDER BY category, key;
```

The database tables must be populated with data according to the user roles to have a proper functioning flow.

When the database is cleared, all missing database tables (organizations, departments, roles, permissions, etc.) must be created. The organizations table must include the missing code column. All tables must be populated with comprehensive demo data. After database restoration, the application must be tested.

When inserting into the `customers` table, ensure that the `kyc_status` value is a valid enum value. Valid `kyc_status` values:
- `'pending'` - For customers awaiting KYC verification
- `'verified'` - For customers with completed KYC
- `'rejected'` - For customers with rejected KYC
- `'under_review'` - For customers under KYC review

When inserting into the `customers` table, ensure that the `organization_id` column is included, as it has a NOT NULL constraint. The `customers` table schema contains the following:
- `id` (auto-generated)
- `full_name` (required)
- `kyc_status` (required, defaults to 'pending')
- `organization_id` (required)

The `customers` table has `mobile` column for phone number, not `phone`. The `customers` table has `dob` column for date of birth, not `date_of_birth`. `user_id` is optional (nullable).

When inserting into the `cases` table, ensure that the `organization_id`, `case_number`, `customer_id`, and `product_id` columns are included, as they have NOT NULL constraints. The `status` column defaults to `'open'`, and the `priority` column defaults to `'medium'`. The `case_number` should follow a realistic case numbering system (e.g., `CASE-2024-001`).

**The valid enum values for `case_status` were: `'new'`, `'in-progress'`, `'review'`, `'approved'`, `'rejected'`, and `'on-hold'`. Use these values when querying or updating the `cases` table.** The actual valid enum values as of 2025-09-24 are: `'open'`, `'in_progress'`, `'closed'`, and `'rejected'`.
Valid `priority` enum values are `low`, `medium`, and `high`.

When inserting into the `documents` table, be aware of the following:
- The schema uses `customer_id` instead of `case_id`.
- `organization_id` is a required field.
- Fields like `file_name`, `file_path`, `file_size`, `file_type`, `uploaded_at`, and `notes` do not exist.
- The schema has fields like `file_id`, `uploaded_by`, `verified_by`, and `verified_on`.

When writing Supabase queries involving the `documents` table, ensure that you use `customer_id` rather than `loan_application_id` to filter documents.

When encountering HTTP 400 or 406 errors when querying users by `auth_id` or direct `id`, especially for the super admin user, it indicates that the queries are not correctly handling database-only users. The following rules apply:

- **Dual Authentication Handling:** Methods must handle both Supabase Auth users (identified by `auth_id`) and database-only users (identified by direct `id`).
- **getUserById Method:** Fix `getUserById` and related methods to properly handle database-only users. Attempt to find the user by `auth_id` first, and if not found, then attempt to find the user by their direct `id`.
- **Team Member Fetching:** Ensure team member fetching handles a mix of Supabase Auth and database-only users.
- **ID Format Detection**: Use regex `/^\d+$/` to detect numeric IDs vs UUIDs.
- **Direct Database ID Usage**: When a numeric ID is detected, use it directly without attempting `auth_id` lookup.
- **Empty User ID Handling**: In the `getWorkloadTasks` and `getUserById` methods, return an empty array or null respectively if no `userId` is provided, or if the `userId` is an empty string.

When displaying user initials based on the user's full name, check for null or undefined names and provide a fallback initial.

When accessing the `length` property of a potentially undefined variable (e.g., `approvalQueue`), use the optional chaining operator (`?.`) to prevent errors. For example: `approvalQueue?.length || 0`.

When using the `useApprovalQueue` hook, destructure the returned data as `{ data: approvalQueue, loading, error, refetch }`, as the hook returns the approval queue data under the `data` property.

When fetching cases with compliance issues, use the `'on-hold'` status instead of `'compliance-issue'`.

When populating the database with initial demo data, use the following guidelines regarding `case_status`:

The valid enum values for `case_status` are: `open`, `in_progress`, `closed`, `rejected`. Valid `priority` enum values are `low`, `medium`, and `high`.
If these values are not valid in your specific database, use the following SQL to determine valid values:

```sql
-- Get all enum values for data setup (fixed version)
SELECT 'case_status_t' as enum_name, unnest(enum_range(NULL::case_status_t))::text as enum_value
UNION ALL

-- 2. case_priority_t values  
SELECT 'case_priority_t', unnest(enum_range(NULL::case_priority_t))::text
UNION ALL

-- 3. kyc_status_t values
SELECT 'kyc_status_t', unnest(enum_range(NULL::kyc_status_t))::text
UNION ALL

-- 4. doc_status_t values
SELECT 'doc_status_t', unnest(enum_range(NULL::doc_status_t))::text
UNION ALL

-- 5. file_type_t values
SELECT 'file_type_t', unnest(enum_range(NULL::file_type_t))::text
UNION ALL

-- 6. task_priority_t values
SELECT 'task_priority_t', unnest(enum_range(NULL::task_priority_t))::text
UNION ALL

-- 7. task_status_t values
SELECT 'task_status_t', unnest(enum_range(NULL::task_status_t))::text
UNION ALL

-- 8. user_status_t values
SELECT 'user_status_t', unnest(enum_range(NULL::user_status_t))::text
UNION ALL

-- 9. webhook_status_t values
SELECT 'webhook_status_t', unnest(enum_range(NULL::webhook_status_t))::text

ORDER BY enum_name, enum_value;
```

**The valid enum values fetched from the above SQL query for the demo data as of 2025-09-23 are:**
- **Case Status:** `open`, `in_progress`, `closed`, `rejected`
- **Case Priority:** `high`, `medium`, `low`
- **KYC Status:** `verified`, `pending`, `rejected`
- **Document Status:** `verified`, `pending`, `rejected`
- **Task Status:** `open`, `in_progress`, `completed`, `overdue`
- **Task Priority:** `high`, `normal`, `low`
- **User Status:** `active`, `inactive`
- **File Type:** `csv`, `doc`, `docx`, `jpeg`, `jpg`, `pdf`, `png`, `txt`, `xls`, `xlsx`
- **Webhook Status:** `failed`, `pending`, `processed`

When populating the database with initial demo data, ensure that the `cases` table includes a non-null `case_number` column. The `case_number` should follow a realistic case numbering system (e.g., `CASE-2024-001`).

To ensure unique keys for map operations in React components, use a combination of table name, row ID, and column name when generating keys. For example, use `${selectedTable}-row-${row.id || index}` for table rows,`${selectedTable}-col-${column.name}` for column headers, and `${selectedTable}-cell-${row.id || index}-${column.name}` for table cells. When mapping over arrays, always handle potential null or undefined data by using the optional chaining operator (?.). For example, use `(tableData?.data || []).map()` and `(tableSchema?.columns || []).map()`.

When a component has real-time auto-refresh functionality, make the auto-refresh optional with user controls. Provide a toggle checkbox for enabling/disabling auto-refresh and a clear visual indicator when auto-refresh is active. The default state should be auto-refresh OFF.

When the keys are not unique in a map function, it can lead to React key warnings. Ensure unique keys for table rows by using `${selectedTable}-row-${row.id || index}`. For table columns use `${selectedTable}-col-${column.name}` and for table cells use `${selectedTable}-cell-${row.id || index}-${column.name}`. Also ensure that the data being mapped over is not null or undefined by using `(tableData?.data || []).map()` and `(tableSchema?.columns || []).map()`. When using checkboxes, only allow selection of rows with valid IDs. The select all checkbox should handle undefined data gracefully.

#### Database Management System

The following rules apply to the Database Management System in the super admin dashboard:

-   The Super Admin must be able to view and manage all tables and their data.
-   Every table must be updatable, allowing adding, editing, and deleting entries according to the project's functionality, roles, and flow.
-   The system must handle missing tables gracefully and provide a way to create them.
-   The system must have real-time auto-refresh functionality.
-   The system must have unique keys for table rows, column headers, and table cells.
-   The system must handle null or undefined data gracefully.
-   CRUD operations wherever needed should be implemented using modals. Ensure these modals add, edit, and delete rows from Supabase.

The DatabaseManagement component now uses `row.id || index` for table rows, but if some rows don't have an `id` field or have duplicate IDs, this could cause React key warnings. Ensure unique keys for table rows by using `${selectedTable}-row-${row.id || index}`.

The keys for column headers are not unique. Use `${selectedTable}-col-${column.name}` to prevent conflicts between tables.

The keys for table cells are not unique. Use `${selectedTable}-cell-${row.id || index}-${column.name}` to prevent conflicts between tables.

Ensure data is not null or undefined by using `(tableData?.data || []).map()` and `(tableSchema?.columns || []).map()`.

The "select all" checkbox logic must handle undefined data gracefully.

### 1. Smart Table Detection

-   **Missing Table Detection**: Properly identifies tables that don't exist vs. tables with errors
-   **Core Table Identification**: Distinguishes between essential tables and optional advanced feature tables
-   **Visual Status Indicators**:
    -   üü¢ **Active** (green) - Table exists and working
    -   üü° **Missing** (yellow) - Table doesn't exist yet
    -   üî¥ **Error** (red) - Table has issues

### 2. Enhanced Dashboard Metrics

-   **5 Metric Cards**: Total Tables, Active Tables, Missing Tables, Total Records, Health Score
-   **Missing Tables Alert**: Yellow warning card when tables are missing
-   **Better Health Scoring**: Separate core health score tracking

### 3. Missing Tables Helper

-   **SQL Script Generator**: Automatically creates SQL scripts to build missing tables
-   **One-Click Copy**: Copy SQL scripts to clipboard
-   **Download Option**: Download SQL files for manual execution
-   **Step-by-step Instructions**: Clear guidance for creating tables in Supabase
-   **Table Dependencies**: Proper creation order for related tables

### 4. Improved Table Browser

-   **Core Table Badges**: Visual indicators for essential vs. optional tables
-   **Disabled Missing Tables**: Can't accidentally select missing tables
-   **Better Table Info**: Shows "Table not found" for missing tables
-   **Sorted Display**: Core tables appear first

### 5. Professional Error Handling

-   **No More 404s**: Gracefully handles missing tables without console errors
-   **Smart Status Detection**: Distinguishes between "doesn't exist" and "permission denied"
-   **User-Friendly Messages**: Clear explanations instead of raw error codes

### 6. Auto-Refresh

-   Auto-refresh is now **OFF by default**. Provide a toggle checkbox for enabling/disabling auto-refresh and a clear visual indicator when auto-refresh is active.
-   You can enable/disable it with a checkbox.
-   Visual "Live" indicator when auto-refresh is active

#### Dynamic Table Management

To support dynamic table management with CRUD operations, the following priority of tables is defined. Full CRUD operations must be implemented for all tables, based on the roles and permissions defined in the system.

**Priority 1: Core Business Tables**
1.  **organizations** - ‚úÖ Full CRUD
2.  **users** - ‚úÖ Full CRUD
3.  **customers** - ‚úÖ Full CRUD
4.  **cases** - ‚úÖ Full CRUD
5.  **documents** - ‚úÖ Full CRUD
6.  **products** - ‚úÖ Full CRUD
7.  **departments** - ‚úÖ Full CRUD
8.  **tasks** - ‚úÖ Full CRUD
9.  **notifications** - ‚úÖ Full CRUD

**Priority 2: Configuration Tables**
1.  **document_types** - ‚úÖ Full CRUD
2.  **roles** - ‚úÖ Full CRUD
3.  **permissions** - ‚úÖ Full CRUD
4.  **task_types** - ‚úÖ Full CRUD
5.  **employment_types** - ‚úÖ Full CRUD
6.  **system_settings** - ‚úÖ Full CRUD

**Priority 3: Advanced Features**
1.  **workflow_stages** - ‚úÖ Full CRUD
2.  **workflow_transitions** - ‚úÖ Full CRUD
3.  **compliance_issues** - ‚úÖ Full CRUD
4.  **approval_queues** - ‚úÖ Full CRUD
5.  **feature_flags** - ‚úÖ Full CRUD

### Supabase Subscriptions

Supabase subscriptions are real-time connections that automatically push database changes to the frontend application the moment they happen. They provide "live updates" for database records.

The application uses fully functional subscriptions. The following tables have real-time updates:
- `cases` table: Case status changes, new cases, updates
- `documents` table: New documents, file changes, status updates
- `whatsapp_messages` table: New messages, delivery status
- `tasks` table: New tasks, status changes, assignments
- Multiple tables: Live statistics for dashboard stats
- `users` table: User additions, status changes for User Management
- `organizations` table: Org updates, user assignments for Organization Management
- `files` table: Uploads, downloads, changes for File Management
- `permissions/roles` tables: Access control changes for Permissions/Roles
- `loan_applications` table: Status changes, new submissions for Loan Applications

Subscription setup occurs in the database service. For example:
```typescript
// FROM: src/services/supabase-database.ts
static subscribeToCases(callback: (payload: any) => void) {
  return supabase
    .channel('cases')                    // Creates a subscription channel
    .on('postgres_changes',             // Listens for database changes
      { 
        event: '*',                     // All database events (INSERT, UPDATE, DELETE)
        schema: 'public', 
        table: SUPABASE_TABLES.CASES   // Which table to watch
      },
      callback                          // What function to call when change detected
    )
    .subscribe();                       // Activate the subscription
}
```

React hook integration occurs as follows:
```typescript
// FROM: src/hooks/useDashboardData.ts (Lines 215-224)
export function useCases(filters) {
  // ... existing state and fetchData ...

  // Set up real-time subscription
  useEffect(() => {
    const subscription = SupabaseDatabaseService.subscribeToCases((payload) => {
      console.log('Case real-time update:', payload);
      fetchData(); // Refetch on any change
    });

    return () => {
      subscription.unsubscribe(); // Clean up subscription
    };
  }, [fetchData]);
}
```

The complete flow in action is:
USER ACTION ‚Üí DATABASE UPDATE ‚Üí SUBSCRIPTION TRIGGER ‚Üí UI AUTO-UPDATE

#### CRUD Modals

The application uses reusable modal components for CRUD operations, integrated with Supabase for real-time data persistence.

-   **Core Modal Components:**
    -   `src/components/ui/Modal.tsx` - Base modal framework
    -   `src/components/ui/FormModal.tsx` - Form handling and CRUD operations wrapper. This component supports loading states, validation, and real-time interaction. It utilizes `ValidatedInput`, `ValidatedSelect`, and `ValidatedTextarea` from `src/components/ui/FormField.tsx`.
-   **Entity-Specific CRUD Modals:**
    -   `src/components/modals/UserManagementModal.tsx` - User CRUD with role/org/department management
    -   `src/components/modals/CaseManagementModal.tsx` - Loan case creation and management
    -   `src/components/modals/DocumentManagementModal.tsx` - Document upload and file handling
    -   `src/components/modals/OrganizationManagementModal.tsx` - Organization setup and configuration

All components ensure that every **Save**, **Update**, or **Delete** operation goes directly to the Supabase database and retains the real state. They can be integrated into any page within the Veriphy Bank application to provide immediate CRUD functionality wherever data management is required.

The Database Management System should use these CRUD Modals.

## Main User Roles

The main roles are all below super admin. The super admin page is only for initial setup. The system must focus on the flow and functionality of other roles and perfect it. Every navigation and data should work with CRUD wherever required. There should not be any dead pages or no content errors for these main roles.

## TO-DO List

-   Analyze existing dashboard and navigation functionality for each role (salesperson, manager, credit-ops, compliance, admin, auditor)
-   Identify missing components and dead pages for each role's navigation flow
-   Ensure CRUD operations are implemented in all key components with Supabase integration
-   Fix dead navigation links and ensure all role-based navigation works properly
-   Verify all components use real Supabase data instead of mock data
-   Improve navigation UX for all user roles with proper loading states and error handling

## WORKFLOW & RELEASE RULES

When integrating Supabase live data, follow these steps:
1. Analyze current Supabase integration status across all components.
2. Review Supabase database service methods and functionality.
3. Check all components for live data integration.
4. Identify components missing Supabase integration.
5. Generate comprehensive integration status report.

After integrating components, the system is ready for:
1. **Navigation Integration** - Add these components to the main app routing
2. **Advanced Features** - Implement pagination, bulk operations, and advanced filtering
3. **Testing** - Add comprehensive unit and integration tests
4. **Performance Optimization** - Implement caching strategies and query optimization
5. **User Experience** - Add advanced UI features like drag-and-drop, bulk actions, etc.

When setting up the system as a super admin, follow these steps:
1. Login as Super Admin ‚Üí Access Super Admin Dashboard
2. Click "System Setup" ‚Üí Opens the setup wizard
3. Step 1: Organizations ‚Üí Create your first organization
4. Step 2: Departments ‚Üí Set up department structure
5. Step 3: Products ‚Üí Configure loan products
6. Step 4: Workflows ‚Üí (Coming soon)

When setting up the system as an admin:
1. Login as Admin ‚Üí Access Admin Dashboard
2. Use the same components to set up within their organization
3. Create users and assign to departments
4. Configure organization-specific settings

To fix import errors:
1. Check the exact file path and name.
2. Correct the import statement to match the actual location.

The Super Admin setup involves these steps:
1. **Organization Management** - Create/edit organizations
2. **Department Management** - Set up departments for each organization
3. **Role & Permission Management** - Define roles and permissions
4. **Product Management** - Create loan products
5. **Document Type Management** - Define required documents
6. **Workflow Setup** - Create approval workflows

The Admin setup (Organization Level) involves these steps:
1. **User Management** - Create users within their organization
2. **Department Assignment - Assign users to departments
3. **Task Category Setup** - Create task categories
4. **Compliance Setup** - Set up compliance rules

Multiple methods are available for Super Admin setup:
1. **Quick Setup (Recommended)**
   1. Start your app: `npm run dev`
   2. Go to setup page: `http://localhost:5173/setup`
   3. Click "Create Super Admin"
   4. Login with: `superadmin@veriphy.com` (any password)
2. **Direct Database**
   1. Open Supabase Dashboard
   2. Go to Table Editor ‚Üí users table
   3. Insert the super admin record (see SUPER_ADMIN_SETUP.md)
4. **Browser Console**
   1. Open your app in browser
   2. Press F12 ‚Üí Console
   3. Run the setup script (see SUPER_ADMIN_SETUP.md)

To resolve project issues, follow these steps:
1.  Run the build command: `npm run build`
2.  Check for TypeScript compilation errors: `npx tsc --noEmit`
3.  Run the linter: `npm run lint`
4.  Address linting errors systematically, prioritizing the following:
    - Remove unused variables and imports.
    - Replace 'any' types with proper TypeScript types.
    - Ensure variables are correctly declared as `const` instead of `let` where appropriate.
    - Fix lexical declarations in case blocks.
    - Correct parsing errors.
    - Fix unnecessary escape characters in regex.

The following step-by-step plan must be followed to populate all tables with demo data:
1. Analyze all Supabase tables and their relationships.
2. Map user roles to their responsibilities and permissions.
3. Design comprehensive data flow with realistic demo data.
4. Create SQL scripts to populate all tables with demo data.
5. Execute the population scripts in Supabase.
6. Test the application with populated data.

To restore the database after accidentally using the clear database feature in the super admin dashboard, follow these steps:
1. Create all missing database tables (organizations, departments, roles, permissions, etc.).
2. Fix the organizations table to include the missing code column.
3. Populate all tables with comprehensive demo data.
4. Test the application after database restoration.

To check if the table structure is fixed:
## **üîç METHOD 1: Check Table Structure with DESCRIBE (PostgreSQL)**

Run these queries in your Supabase SQL Editor to check each table structure:

```sql
-- Check if all required columns exist in each table
SELECT column_name, data_type, is_nullable, column_default 
FROM information_schema.columns 
WHERE table_name = 'organizations' 
ORDER BY ordinal_position;

SELECT column_name, data_type, is_nullable, column_default 
FROM information_schema.columns 
WHERE table_name = 'users' 
ORDER BY ordinal_position;

SELECT column_name, data_type, is_nullable, column_default 
FROM information_schema.columns 
WHERE table_name = 'customers' 
ORDER BY ordinal_position;

SELECT column_name, data_type, is_nullable, column_default 
WHERE table_name = 'cases' 
ORDER BY ordinal_position;

SELECT column_name, data_type, is_nullable, column_default 
FROM information_schema.columns 
WHERE table_name = 'notifications' 
ORDER BY ordinal_position;
```

## **üîç METHOD 2: Check All Tables Exist**

```sql
-- Check if all required tables exist
SELECT table_name 
FROM information_schema.tables 
WHERE table_schema = 'public' 
AND table_name IN (
  'organizations', 'departments', 'roles', 'permissions', 'role_permissions',
  'users', 'user_roles', 'products', 'document_types', 'customers', 
  'cases', 'documents', 'tasks', 'logs', 'notifications', 'system_settings'
)
ORDER BY table_name;
```

## **üîç METHOD 3: Check Specific Columns Exist**

```sql
-- Check for specific columns that were missing
SELECT 
  table_name,
  column_name,
  data_type
FROM information_schema.columns 
WHERE table_schema = 'public' 
AND (
  (table_name = 'organizations' AND column_name = 'code') OR
  (table_name = 'departments' AND column_name = 'code') OR
  (table_name = 'users' AND column_name IN ('full_name', 'role', 'organization_id', 'department_id', 'mobile')) OR
  (table_name = 'customers' AND column_name IN ('full_name', 'phone', 'email', 'pan_number', 'aadhaar_number')) OR
  (table_name = 'cases' AND column_name IN ('case_number', 'loan_amount', 'status')) OR
  (table_name = 'notifications' AND column_name = 'is_read')
)
ORDER BY table_name, column_name;
```

## **üîç METHOD 4: Test Data Insertion**

Try inserting test data to see if the structure works:

```sql
-- Test inserting into organizations (should work if structure is correct)
INSERT INTO organizations (name, code, description, email, phone) 
VALUES ('Test Org', 'TEST', 'Test Organization', 'test@test.com', '+91-1234567890')
ON CONFLICT (code) DO NOTHING;

-- Test inserting into users (should work if structure is correct)
INSERT INTO users (email, password_hash, full_name, mobile, role, organization_id, department_id) 
VALUES ('test@test.com', 'test123', 'Test User', '+91-1234567890', 'test_role', 1, 1)
ON CONFLICT (email) DO NOTHING;
```

## **üîç METHOD 5: Check Foreign Key Relationships**

```sql
-- Check foreign key constraints
SELECT 
  tc.table_name, 
  kcu.column_name, 
  ccu.table_name AS foreign_table_name,
  ccu.column_name AS foreign_column_name 
FROM information_schema.table_constraints AS tc 
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
  AND tc.table_schema = kcu.table_schema
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
  AND ccu.table_schema = ccu.table_schema
WHERE tc.constraint_type = 'FOREIGN KEY' 
AND tc.table_schema = 'public'
ORDER BY tc.table_name;
```

## **üîç METHOD 6: Comprehensive Verification Script**

Here's a complete verification script you can run:

```sql
-- COMPREHENSIVE TABLE STRUCTURE VERIFICATION
-- Run this to check if all tables and columns are properly created

DO $$
DECLARE
    missing_tables TEXT[] := ARRAY[]::TEXT[];
    missing_columns TEXT[] := ARRAY[]::TEXT[];
    table_name TEXT;
    column_name TEXT;
    expected_tables TEXT[] := ARRAY[
        'organizations', 'departments', 'roles', 'permissions', 'role_permissions',
        'users', 'user_roles', 'products', 'document_types', 'customers', 
        'cases', 'documents', 'tasks', 'logs', 'notifications', 'system_settings'
    ];
    expected_columns JSONB := '{
        "organizations": ["id", "name", "code", "description", "email", "phone"],
        "users": ["id", "email", "password_hash", "full_name", "role", "organization_id", "department_id", "mobile"],
        "customers": ["id", "user_id", "full_name", "phone", "email", "pan_number", "aadhaar_number"],
        "cases": ["id", "case_number", "customer_id", "assigned_to", "loan_type", "loan_amount", "status"],
        "notifications": ["id", "user_id", "type", "title", "message", "is_read"]
    }';
BEGIN
    -- Check for missing tables
    FOREACH table_name IN ARRAY expected_tables
    LOOP
        IF NOT EXISTS (
            SELECT 1 FROM information_schema.tables 
            WHERE table_schema = 'public' AND table_name = table_name
        ) THEN
            missing_tables := array_append(missing_tables, table_name);
        END IF;
    END LOOP;
    
    -- Check for missing columns
    FOR table_name, column_name IN 
        SELECT